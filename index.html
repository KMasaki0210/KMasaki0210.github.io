<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Masaki Kawamura</title>
    <link rel="icon" type="image/png" href="images/icon.png" />
    <link rel="stylesheet" href="css/style.css" />
  </head>
  <body>
    <main class="container">
      <header class="hero">
        <div class="hero-image-wrap">
          <img
            src="images/image.png"
            alt="Masaki Kawamura profile photo"
          />
        </div>
        <div>
          <h1>Masaki Kawamura</h1>
          <p class="subtitle">M1 @ Rio Yokota Lab, Institute of Science Tokyo (CLIP / LLM / VLM / VLA)</p>
          <div class="links">
            <a class="btn" href="https://x.com/Masakichi333210" target="_blank" rel="noreferrer">X</a>
            <a class="btn" href="https://www.linkedin.com/in/masaki-kawamura-0806a7361/" target="_blank" rel="noreferrer">LinkedIn</a>
            <a class="btn" href="https://scholar.google.com/citations?user=vOOChZ4AAAAJ" target="_blank" rel="noreferrer">Google Scholar</a>
            <a class="btn" href="https://github.com/KMasaki0210" target="_blank" rel="noreferrer">GitHub</a>
            <a class="btn" href="https://zenn.dev/masakichi210" target="_blank" rel="noreferrer">Zenn</a>
            <!--　GoogleScholar, CV　-->
          </div>
        </div>
      </header>

      <div class="grid">
        <section>
          <h2>About</h2>
          <p>
            I'm Masaki Kawamura. 
            Machine Learning Engineer specializing in the research and development of Large Language Models (LLMs) and Vision-Language Models (VLMs). Currently, my work focuses on data pipeline construction for the <a class="music-link" href="https://swallow-llm.github.io/index.en.html" target="_blank" rel="noreferrer">Swallow project</a> and the application of reinforcement learning (RL) to LLMs and VLMs. 
            I have a solid research background in CLIP models and am increasingly focused on the development of Vision-Language-Action (VLA) models for real-world applicability. 
            I graduated early from my undergraduate program, earning my Bachelor’s degree one year ahead of schedule. 
          </p>
        </section>

        <section>
          <h2>Education</h2>
          <ul>
            <li><strong>Apr 2025 - Mar 2027 (expected)</strong> Master of Science in Computer Science, Institute of Science Tokyo (Formerly Tokyo Institute of Technology)</li>
            <li><strong>Apr 2022 - Mar 2025</strong> Bachelor of Science in Computer Science, Institute of Science Tokyo (Formerly Tokyo Institute of Technology)</li>
            <li><strong>Apr 2019 - Mar 2022</strong> Sapporo North High School</li>
          </ul>
        </section>

        <section>
          <h2>Experience</h2>
          <ul>
            <li><strong>Feb 2025 - Present</strong> Researcher (Intern), SB Intuitions</li>
            <li><strong>Dec 2024 - Present</strong> Technical Assistant and Research Assistant, National Institute of Advanced Industrial Science and Technology (AIST)</li>
            <li><strong>Aug 2025</strong> Backend Engineer(Intern), Money Forward</li>
            <li><strong>Nov 2024 - Mar 2025</strong> Research Assistant, National Institute of Infomatics (NII)</li>
          </ul>
        </section>

        <section>
          <h2>Publication</h2>
          <ul>
            <li>
              <strong>Masaki Kawamura</strong>, Nakamasa Inoue, Rintaro Yanagi, Hirokatsu Kataoka, Rio Yokota.
              PowerCLIP: Powerset Alignment for Contrastive Pre-Training.
              <a class="arxiv-btn" href="https://arxiv.org/abs/2511.23170" target="_blank" rel="noreferrer">arXiv</a>
            </li>
            <li>
              Kazuki Fujii, Yukito Tajima, Sakae Mizuki, <strong>Masaki Kawamura</strong>, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Masanari Oi, Taishi Nakamura, Takumi Okamoto, Shigeki Ishida, Kakeru Hattori, Youmi Ma, Hiroya Takamura, Rio Yokota, Jun Sakuma, Naoaki Okazaki.
              Rewriting Pre-Training Data Boosts LLM Performance in Math and Code.
              International Conference on Learning Representations (ICLR), 2026.
              <a class="arxiv-btn" href="https://arxiv.org/abs/2505.02881" target="_blank" rel="noreferrer">arXiv</a>
            </li>
            <li>
              Taishi Nakamura, Satoki Ishikawa, <strong>Masaki Kawamura</strong>, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota.
              Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks.
              International Conference on Learning Representations (ICLR) Oral, 2026.
              <a class="arxiv-btn" href="https://arxiv.org/abs/2508.18672" target="_blank" rel="noreferrer">arXiv</a>
            </li>
          </ul>
        </section>

        <section class="full">
          <h2>Hobbies</h2>
          <ul>
            <li>Sports: Soccer (Atletico Madrid), Busketball</li>
            <li>
              Music:
              <a class="music-link" href="https://www.asiankung-fu.com/s/n80/?ima=2709" target="_blank" rel="noreferrer">ASIAN KUNG-FU GENERATION</a>,
              <a class="music-link" href="https://sakanaction.jp/" target="_blank" rel="noreferrer">Sakanaction</a>
            </li>
          </ul>
        </section>
      </div>

      <footer>© 2026 Masaki Kawamura | Last updated: Feb 2026</footer>
    </main>
  </body>
</html>